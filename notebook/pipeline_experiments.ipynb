{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIALAB PIPELINE EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import warnings\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import sklearn.ensemble as sk_ensemble\n",
    "import numpy as np\n",
    "import pymia.data.conversion as conversion\n",
    "import pymia.evaluation.writer as writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mialab.data.structure as structure\n",
    "import mialab.utilities.file_access_utilities as futil\n",
    "import mialab.utilities.pipeline_utilities as putil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADING_KEYS = [structure.BrainImageTypes.T1w,\n",
    "                structure.BrainImageTypes.T2w,\n",
    "                structure.BrainImageTypes.GroundTruth,\n",
    "                structure.BrainImageTypes.BrainMask,\n",
    "                structure.BrainImageTypes.RegistrationTransform]  # the list of data we will load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "# MAIN FUNCTION\n",
    "\n",
    "def main(result_dir: str, data_atlas_dir: str, data_train_dir: str, data_test_dir: str, \n",
    "         config_dict: dict = None):\n",
    "    \"\"\"Brain tissue segmentation using decision forests.\n",
    "\n",
    "    The main routine executes the medical image analysis pipeline:\n",
    "\n",
    "        - Image loading\n",
    "        - Registration\n",
    "        - Pre-processing\n",
    "        - Feature extraction\n",
    "        - Decision forest classifier model building\n",
    "        - Segmentation using the decision forest classifier model on unseen images\n",
    "        - Post-processing of the segmentation\n",
    "        - Evaluation of the segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    # Default configuration if none provided\n",
    "    if config_dict is None:\n",
    "        config_dict = {\n",
    "            'preprocessing': {\n",
    "                'skullstrip_pre': True,\n",
    "                'normalization_pre': True,\n",
    "                'registration_pre': True,\n",
    "                'coordinates_feature': True,\n",
    "                'intensity_feature': True,\n",
    "                'gradient_intensity_feature': True\n",
    "            },\n",
    "            'postprocessing': {\n",
    "                'simple_post': True\n",
    "            },\n",
    "            'forest': {\n",
    "                'n_estimators': 10,\n",
    "                'max_depth': 10,\n",
    "                'max_features': None\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Extract configuration\n",
    "    pre_process_params = config_dict.get('preprocessing', {})\n",
    "    post_process_params = config_dict.get('postprocessing', {})\n",
    "    forest_params = config_dict.get('forest', {})\n",
    "\n",
    "    # load atlas images\n",
    "    putil.load_atlas_images(data_atlas_dir)\n",
    "\n",
    "    print('-' * 5, 'Training...')\n",
    "\n",
    "    # crawl the training image directories\n",
    "    crawler = futil.FileSystemDataCrawler(data_train_dir,\n",
    "                                          LOADING_KEYS,\n",
    "                                          futil.BrainImageFilePathGenerator(),\n",
    "                                          futil.DataDirectoryFilter())\n",
    "\n",
    "    # load images for training and pre-process\n",
    "    images = putil.pre_process_batch(crawler.data, pre_process_params, multi_process=False)\n",
    "\n",
    "    # generate feature matrix and label vector\n",
    "    data_train = np.concatenate([img.feature_matrix[0] for img in images])\n",
    "    labels_train = np.concatenate([img.feature_matrix[1] for img in images]).squeeze()\n",
    "\n",
    "    # Setup random forest with configurable parameters\n",
    "    n_features = images[0].feature_matrix[0].shape[1]\n",
    "    max_features = forest_params.get('max_features', n_features)\n",
    "    if max_features is None:\n",
    "        max_features = n_features\n",
    "    \n",
    "    forest = sk_ensemble.RandomForestClassifier(\n",
    "        max_features=max_features,\n",
    "        n_estimators=forest_params.get('n_estimators', 10),\n",
    "        max_depth=forest_params.get('max_depth', 10),\n",
    "        random_state=42,  # For reproducibility\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "\n",
    "    print(f'Training Random Forest with {forest_params.get(\"n_estimators\", 10)} trees, '\n",
    "          f'max_depth={forest_params.get(\"max_depth\", 10)}, '\n",
    "          f'max_features={max_features} (out of {n_features} total features)')\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    forest.fit(data_train, labels_train)\n",
    "    print(' Time elapsed:', timeit.default_timer() - start_time, 's')\n",
    "\n",
    "    # create a result directory with timestamp\n",
    "    t = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    result_dir = os.path.join(result_dir, t)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    # Save configuration used for this experiment\n",
    "    import json\n",
    "    with open(os.path.join(result_dir, 'experiment_config.json'), 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    print('-' * 5, 'Testing...')\n",
    "\n",
    "    # initialize evaluator\n",
    "    evaluator = putil.init_evaluator()\n",
    "\n",
    "    # crawl the training image directories\n",
    "    crawler = futil.FileSystemDataCrawler(data_test_dir,\n",
    "                                          LOADING_KEYS,\n",
    "                                          futil.BrainImageFilePathGenerator(),\n",
    "                                          futil.DataDirectoryFilter())\n",
    "\n",
    "    # load images for testing and pre-process\n",
    "    pre_process_params['training'] = False\n",
    "    images_test = putil.pre_process_batch(crawler.data, pre_process_params, multi_process=False)\n",
    "\n",
    "    images_prediction = []\n",
    "    images_probabilities = []\n",
    "\n",
    "    for img in images_test:\n",
    "        print('-' * 10, 'Testing', img.id_)\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        predictions = forest.predict(img.feature_matrix[0])\n",
    "        probabilities = forest.predict_proba(img.feature_matrix[0])\n",
    "        print(' Time elapsed:', timeit.default_timer() - start_time, 's')\n",
    "\n",
    "        # convert prediction and probabilities back to SimpleITK images\n",
    "        image_prediction = conversion.NumpySimpleITKImageBridge.convert(predictions.astype(np.uint8),\n",
    "                                                                        img.image_properties)\n",
    "        image_probabilities = conversion.NumpySimpleITKImageBridge.convert(probabilities, img.image_properties)\n",
    "\n",
    "        # evaluate segmentation without post-processing\n",
    "        evaluator.evaluate(image_prediction, img.images[structure.BrainImageTypes.GroundTruth], img.id_)\n",
    "\n",
    "        images_prediction.append(image_prediction)\n",
    "        images_probabilities.append(image_probabilities)\n",
    "\n",
    "    # post-process segmentation and evaluate with post-processing\n",
    "    images_post_processed = putil.post_process_batch(images_test, images_prediction, images_probabilities,\n",
    "                                                     post_process_params, multi_process=True)\n",
    "\n",
    "    for i, img in enumerate(images_test):\n",
    "        evaluator.evaluate(images_post_processed[i], img.images[structure.BrainImageTypes.GroundTruth],\n",
    "                           img.id_ + '-PP')\n",
    "\n",
    "        # save results\n",
    "        sitk.WriteImage(images_prediction[i], os.path.join(result_dir, images_test[i].id_ + '_SEG.mha'), True)\n",
    "        sitk.WriteImage(images_post_processed[i], os.path.join(result_dir, images_test[i].id_ + '_SEG-PP.mha'), True)\n",
    "\n",
    "    # use two writers to report the results\n",
    "    os.makedirs(result_dir, exist_ok=True)  # generate result directory, if it does not exists\n",
    "    \n",
    "    # Write results with custom CSV format that includes experiment metadata\n",
    "    result_file = os.path.join(result_dir, 'results.csv')\n",
    "    write_custom_results_csv(evaluator.results, result_file, config_dict)\n",
    "    \n",
    "    print('\\nSubject-wise results...')\n",
    "    writer.ConsoleWriter().write(evaluator.results)\n",
    "\n",
    "    # report also mean and standard deviation among all subjects\n",
    "    result_summary_file = os.path.join(result_dir, 'results_summary.csv')\n",
    "    functions = {'MEAN': np.mean, 'STD': np.std}\n",
    "    writer.CSVStatisticsWriter(result_summary_file, functions=functions).write(evaluator.results)\n",
    "    print('\\nAggregated statistic results...')\n",
    "    writer.ConsoleStatisticsWriter(functions=functions).write(evaluator.results)\n",
    "\n",
    "    # clear results such that the evaluator is ready for the next evaluation\n",
    "    evaluator.clear()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "# SAVE RESULTS TO CSV FUNCTION\n",
    "\n",
    "def write_custom_results_csv(results, result_file: str, config_dict: dict):\n",
    "    \"\"\"Write results to CSV with consistent format and embedded metadata.\"\"\"\n",
    "    import pandas as pd\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Extract configuration for metadata\n",
    "    preprocessing = config_dict.get('preprocessing', {})\n",
    "    postprocessing = config_dict.get('postprocessing', {})\n",
    "    \n",
    "    # Group flat Results by (id_, label) to collect metrics per row\n",
    "    grouped_results = defaultdict(lambda: defaultdict(dict))  # {id_: {label: {metric: value}}}\n",
    "    \n",
    "    for result in results:  # Iterate the list of Result objects\n",
    "        subject_id = result.id_  # Correct attr: str, e.g., \"118528\" or \"118528-PP\"\n",
    "        label_name = result.label  # str, e.g., \"GREYMATTER\"\n",
    "        metric_name = result.metric  # str, e.g., \"DICE\" or \"HDRFDST95\"\n",
    "        metric_value = result.value  # float, e.g., 0.85\n",
    "        \n",
    "        # Collect into dict (flexible for any metrics)\n",
    "        grouped_results[subject_id][label_name][metric_name] = metric_value\n",
    "    \n",
    "    # Flatten to rows\n",
    "    rows = []\n",
    "    for subject_id, labels_dict in grouped_results.items():\n",
    "        for label_name, metrics_dict in labels_dict.items():\n",
    "            # Extract Dice and Hausdorff (handle naming variations like \"DICE\", \"HD95\")\n",
    "            dice_value = metrics_dict.get('DICE', metrics_dict.get('Dice', None))\n",
    "            hausdorff_value = None\n",
    "            for key in metrics_dict:\n",
    "                if 'Hausdorff' in key or 'HD' in key or 'HDRFDST' in key:\n",
    "                    hausdorff_value = metrics_dict[key]\n",
    "                    break\n",
    "            \n",
    "            row = {\n",
    "                'SUBJECT': subject_id,\n",
    "                'LABEL': label_name,\n",
    "                'DICE': dice_value if dice_value is not None else '',\n",
    "                'HDRFDST': hausdorff_value if hausdorff_value is not None else '',\n",
    "                'normalization': preprocessing.get('normalization_pre', False),\n",
    "                'skull_stripping': preprocessing.get('skullstrip_pre', False),\n",
    "                'registration': preprocessing.get('registration_pre', False),\n",
    "                'postprocessing': postprocessing.get('simple_post', False),\n",
    "                'coordinates_feature': preprocessing.get('coordinates_feature', False),\n",
    "                'intensity_feature': preprocessing.get('intensity_feature', False),\n",
    "                'gradient_intensity_feature': preprocessing.get('gradient_intensity_feature', False)\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Create DataFrame and save as CSV\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(result_file, index=False, sep=',')\n",
    "    \n",
    "    print(f'Results saved to {result_file} with {len(rows)} measurements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\"preprocessing\": {\n",
    "\"skullstrip_pre\": true,\n",
    "\"normalization_pre\": true,\n",
    "\"registration_pre\": true,\n",
    "\"coordinates_feature\": true,\n",
    "\"intensity_feature\": true,\n",
    "\"gradient_intensity_feature\": true\n",
    "},\n",
    "\"postprocessing\": {\n",
    "\"simple_post\": true\n",
    "},\n",
    "\"forest\": {\n",
    "\"n_estimators\": 100,\n",
    "\"max_depth\": null,\n",
    "\"max_features\": \"sqrt\",\n",
    "\"min_samples_split\": 2,\n",
    "\"min_samples_leaf\": 1,\n",
    "\"bootstrap\": true,\n",
    "\"random_state\": 42,\n",
    "\"n_jobs\": -1\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "# RUN MAIN FUNCTION\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"The program's entry point.\"\"\"\n",
    "\n",
    "    script_dir = os.path.dirname(sys.argv[0])\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------\n",
    "    # PARSER FOR ARGUMENTS\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Medical image analysis pipeline for brain tissue segmentation')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--result_dir',\n",
    "        type=str,\n",
    "        default=os.path.normpath(os.path.join(script_dir, './mia-result')),\n",
    "        help='Directory for results.'\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--data_atlas_dir',\n",
    "        type=str,\n",
    "        default=os.path.normpath(os.path.join(script_dir, '../data/atlas')),\n",
    "        help='Directory with atlas data.'\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--data_train_dir',\n",
    "        type=str,\n",
    "        default=os.path.normpath(os.path.join(script_dir, '../data/train/')),\n",
    "        help='Directory with training data.'\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--data_test_dir',\n",
    "        type=str,\n",
    "        default=os.path.normpath(os.path.join(script_dir, '../data/test/')),\n",
    "        help='Directory with testing data.'\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load configuration if provided\n",
    "    config_dict = None\n",
    "    if hasattr(args, 'config_file') and args.config_file:\n",
    "        import json\n",
    "        with open(args.config_file, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "\n",
    "    \n",
    "    main(args.result_dir, args.data_atlas_dir, args.data_train_dir, args.data_test_dir, config_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
